\documentclass[main.tex]{subfiles}
\begin{document}

\section{Path integral basics}

Following \cite[]{zaidiFunctionalMethods1983}.

We start from the space of square-integrable functions \(q(x)\), endowed with a product and an orthonormal basis \(\phi _n\).
We consider (multi-)linear \emph{functionals}, which are maps from the space of square-integrable functions (or from tuples of them) to \(\mathbb{R}\) or \(\mathbb{C}\). 
These can be represented as functions of infinitely many variables, countably so if we use the basis \(\phi _n\), uncountably so if we use the continuous basis \(x\).

A functional \(F[q]\) can be represented as a power series 
%
\begin{align}
F[q] = \sum _{n=0}^{\infty } \frac{1}{n!} \prod_{i=1}^{n} \int \dd{x_i} q(x_i) f(x_1, \dots, x_n)
\,.
\end{align}

Examples of this are the exponential series corresponding to the function \(f(x)\), mapping \(q(x)\) to \(e^{(f, q)}\) where the brackets denote the scalar product in the space, and the Gaussian series corresponding to the kernel \(K(x, y)\), mapping \(q(x)\) to \(e^{(q, K, q)}\), where 
%
\begin{align}
(q, K, q) = \int \dd{x} \dd{y} q(x) q(y) K(x, y)
\,.
\end{align}

\textbf{Functional derivatives} describes how the output of the functional changes as the argument goes from \(q(x)\) to \(q(x) + \eta (x)\), where \(\eta (x)\) is small. 
This will be a linear functional of \(\eta \) to first order, so we define the functional derivative with the expression 
%
\begin{align}
\eval{F[q+\eta ] - F[q]}_{\text{linear order}} = \int \eta (y) \fdv{F}{q (y)} \dd{y}
\,.
\end{align}

The analogy to finite-dimensional spaces is as follows: the functional derivative \(\fdv*{F}{q(y)}\) corresponds to the \emph{gradient} \(\nabla^{i} F\), while the integral in the previous expression corresponds to the \emph{directional derivative} \((\nabla^{i} F) \eta^{j} g_{ij}\).
The metric is present since the gradient is conventionally defined with a vector-like upper index; in our infinite-dimensional space the scalar product is given by the integral.

Practically speaking, the most convenient way to calculate a functional derivative is by taking \(\eta (x)\) to be such that it only differs from zero in a small region near \(y\), and let us define 
%
\begin{align}
\delta \omega = \int \eta (x) \dd{x}
\,.
\end{align}

Then, we define 
%
\begin{align}
\fdv{F}{q(y)} = \lim_{ \delta \omega \to 0} \frac{F[q + \eta ] - F[q]}{ \delta \omega }
\,.
\end{align}

In order for the limit to be computed easily, it is convenient for \(\eta (x)\) to be in the form \(\delta \omega \times \text{fixed function}\),
so that we are only changing the normalization as we shrink \(\delta \omega \).
A common choice is then 
%
\begin{align}
\eta (x) = \delta \omega  \delta (x-y)
\,.
\end{align}

If we apply this procedure to the identity functional \(q \to q\), we find 
%
\begin{align}
\fdv{q(x)}{q(y)} = \lim_{ \delta \omega  \to 0} \frac{q (x) + \delta \omega \delta (x-y) -q (x)}{ \delta \omega } = \delta (x-y)
\,.
\end{align}

The variable \(q\) is one-dimensional, if instead we wanted to consider a multi-dimensional coordinate system \(q_\alpha \) by the same reasoning we would find 
%
\begin{align}
\fdv{q_\alpha (x)}{q_\beta (y)} = \delta_{\alpha \beta } \delta (x-y)
\,.
\end{align}

An example: the functional derivative of a functional \(F_n\) defined by 
%
\begin{align}
F_n[q] = \int f(x_1 , \dots, x_n) q(x_1 )\dots q(x_n) \dd{x_1} \dots \dd{x_n}
\,,
\end{align}
%
where \(f\) is a symmetric function of its arguments, is given by 
%
\begin{align}
\fdv{F_n}{q(y)} = n \int f(x_1, \dots, x_{n-1}, y) q(x_1 )\dots q(x_{n-1}) \dd{x_1 } \dots \dd{x_{n-1}}
\,,
\end{align}
%
a function of \(y\). 

A \textbf{linear transformation} is in the form 
%
\begin{align}
q(x) = \int K(x, y) q'(y) \dd{y}
\,.
\end{align}

If this transformation has an inverse, which is characterized by the kernel \(K^{-1}\), then we must have the orthonormality relation 
%
\begin{align}
\int K(x,y) K^{-1} (y, z) \dd{y} =
\int K^{-1}(x,y) K (y, z) \dd{y} =
\delta (x-z)
\,.
\end{align}

We can do \textbf{Legendre transforms}: if we have a functional \(F\) we can differentiate with respect to the coordinate \(q\) to find 
%
\begin{align}
\fdv{F[q]}{q(x)} = p(x)
\,,
\end{align}
%
in analogy to the momentum in Lagrangian mechanics. Then, we can map \(F[q]\) to a new functional \(G[p]\) which will only depend on the momentum: 
%
\begin{align}
G[p]= F[p] - \int q(x) p(x) \dd{x}
\,.
\end{align}

We can also define functional integration, by 
%
\begin{align}
\int F[q] \qty[ \dd{q}] = \int \hat{F}(\qty{q_i}) \prod_i \dd{q_i}
\,.
\end{align}

On the right-hand side we are using the expression of the functional as a function of infinitely many variables which we discussed above;
we are then integrating over each of the coordinates in this infinite dimensional function space.
The infinite-dimensional measure is also often denoted as \(\mathcal{D}q\). 

This integral will not always exist, however in the cases in which it does we can change variables. 
Let us consider a linear change of variable, whose kernel is \(K(x, y)\), such that (compactly written) \(q = K q'\). 

Then, we want to compute the integral 
%
\begin{align}
\int F[Kq'] \qty[ \dd{Kq'}] 
\,
\end{align}
%
as an integral in \(\qty[ \dd{q}]\): in order to do so, we need to relate the two functional measures. 
We start by expressing both \(q\) and \(q'\) in terms of an orthonormal basis \(\phi _i\): inserting this into the linear transformation law we get 
%
\begin{align}
q(x) &= \int K(x, y) q'(y) \dd{y}  \\
\sum _{i} q_i \phi _i(x) &= \int K(x, y) \sum _{j} q'_j \phi _j (y) \dd{y}  \\
\sum _{i} q_i \underbrace{\int \phi _i (x) \phi _k (x) \dd{x}}_{ \delta_{ik}} 
&= 
\sum _{j} q'_j \underbrace{\int K(x, y) \phi _j (y) \phi _k (x) \dd{y} \dd{x}}_{ k_{jk}}  \\
q_k &= \sum _{j} q'_j k_{jk}
\,.
\end{align}

Then, the measure will transform with the determinant \(\det K = \det k\), which we can now express as an infinite product of the eigenvalues of \(k\): 
%
\begin{align}
\qty[ \dd{q}] = \abs{\pdv{q}{q'}} \qty[ \dd{q'}] = \det K \qty[ \dd{q'}]
\,.
\end{align}

Usually functional integrals cannot be computed analytically; the exception is given by Gaussian integrals, which generalize the finite-dimensional result 
%
\begin{align}
\int_{\mathbb{R}^{n}} \exp(- \frac{1}{2} A_{ij} x_i x_j + i b_j x_j) \dd{x_1 } \dots \dd{x_j} = \sqrt{\frac{(2 \pi)^{n}}{\det A}}
\exp(- \frac{1}{2} (A^{-1})_{ij} b_i b_j)
\,.
\end{align}

Here \(A_{ij}\) is an \(n\)-dimensional real matrix (which WLOG can be taken to be symmetric) while \(b_i\) is an \(n\)-dimensional vector.
The result comes from a transformation of the coordinates according to the finite-dimensional 

This can be interpreted as a ``functional'' (still finite-dimensional, so just a function, but we will generalize soon) of \(b_i\); we write it with an additional normalization \(N\) for convenience:
%
\begin{align}
Z[b] = N \int _{\mathbb{R}^{n}} \exp(- \frac{1}{2} A_{ij} x_i x_j + b_j x_j) \dd{x_1 } \dots \dd{x_j}
= N \sqrt{\frac{(2 \pi )^{n}}{\det A}} \exp(- \frac{1}{2} (A^{-1})_{ij}b_i b_j)
\,,
\end{align}
%
and if we rescale the normalization \(N\) so that \(Z[\vec{0}] = 1 \) we get 
%
\begin{align}
Z[b] = \exp(- \frac{1}{2} (A^{-1})_{ij}b_i b_j)
\,.
\end{align}

The infinite-dimensional generalization of this result amounts to replacing all the sums (expressed implicitly with Einstein notation here) with integrals; also conventionally we change the names of the variables to \(x \to q\), \(A \to K\), \(b \to J\): 
%
\begin{align}
Z[J] &= N \int \mathcal{D}q \exp(- \frac{1}{2} \int \dd{x} \dd{y} K(x, y) q(x) q(y) + i \int \dd{x} q(x) J(x))  \\
&= \exp(- \frac{1}{2} \int \dd{x} \dd{y} J(x) J(y) K^{-1}(x, y))
\,.
\end{align}

Let us now give some examples of applications of this result: \(K(x,y) = \sigma^{-2} \delta (x-y)\) means \(K^{-1}(x,y) = \sigma^2 \delta (x-y) \), so 
%
\begin{align}
Z[J] = \exp(- \frac{\sigma^2}{2} \int \dd{x} J^2(x))
\,.
\end{align}

This, as we shall see, can be used to give us a description of white noise, which is uncorrelated in momentum space.

Let us consider another example, whose physical application is to describe the motion of a massive scalar boson with Lagrangian 
%
\begin{align}
\mathscr{L} = \underbrace{\frac{1}{2} \partial_{\mu } \phi \partial^{\mu } \phi - \frac{1}{2} \mu^2 \phi^2}_{\mathscr{L}_{0}}+ \mathscr{L}_I (\phi )
\,,
\end{align}
%
where the self-interaction term is some non-quadratic function of \(\phi \), often taken to be proportional to  \(\phi^3\)  or \(\phi^{4}\). 

The Feynman path integral corresponding to this Lagrangian is given by the functional 
%
\begin{align}
Z[J] = N \int \mathcal{D} \phi \exp(i \int \mathscr{L}(\phi ) +  J \phi \dd{x} )
\,.
\end{align}

Let us start with the non-interacting case, that is, we compute \(Z_0 \) with only the quadratic term in the Lagrangian. This can be expressed, in the formalism from before, using the kernel 
%
\begin{align}
K(x, y )= (- \square_x - \mu^2 ) \delta (x-y)
\,.
\end{align}

Now, the expression the functional is given in terms of \(K^{-1}\): what is the inverse of this kernel? The definition reduces to 
%
\begin{align}
\int K(x, y) K^{-1}(y, z) \dd{y} &= \delta (x-z)   \\
- \qty(\square_x + \mu^2) K^{-1} (x, z) &= \delta (x-z)
\,,
\end{align}
%
which is readily solved in momentum space, with a \(+i \epsilon \) prescription in order to avoid the pole in the integration: what we find is called the \emph{Green's function}, 
%
\begin{align}
K^{-1}(x, z) = G(x-z) = \frac{1}{(2 \pi )^{4}} \int \frac{e^{-ik \cdot (x-z)}}{k^2 + \mu^2 - i \epsilon } \dd{k} 
\,,
\end{align}
%
so the unperturbed functional reads 
%
\begin{align}
Z_0 [J] = \exp(- \frac{i}{2} \int \dd{x} \dd{y} G(x-y) J(x) J(y))
\,.
\end{align}

% This, evaluated at \(J =0\), can be used to calculate the \emph{propagator}, which quantifies the probability amplitude that a particle localized at a certain position at a certain time will be found at another position, at another time.
% This can be used to compute expectation 

This by itself might not seem very useful, the motion of a free massive boson can be calculated with easier methods.
However, the real power of this path integral is the possibility to write the interacting term perturbatively: the interaction Lagrangian is a function of \(\phi \), which is what we find if we perform a functional integration of the argument of the exponential in \(Z_0 [J]\) with respect to \(J\); so we can express the full functinal as 
%
\begin{align}
Z[J] &= \exp( i\int \dd{x} \mathscr{L}_I\qty( \frac{1}{i} \fdv{}{J(x)}))
\underbrace{\int \mathcal{D} \phi \exp(i \int \dd{x} \qty( \mathscr{L}_0 + J \phi )) }_{= Z_0 [J]}  \\
&= \sum _{n=0}^{\infty } \frac{i^{n}}{n!} \qty[ \int \dd{x} \mathscr{L}_I\qty(\frac{1}{i} \fdv{}{J(x)})]^{n} Z_0 [J]
\,.
\end{align}

We can use this to compute the Green's functions: 
%
\begin{align}
G(x_1 , \dots , x_n) = \eval{\frac{1}{i^{n}} \frac{ \delta Z[J]}{ \delta J (x_1 ) \dots \delta J(x_n)}}_{J = 0}
\,.
\end{align}

\todo[inline]{Review from the PI notes why this makes sense.}

\subsection{The probability density functional}

We can interpret the quantity 
%
\begin{align}
\exp(- \frac{1}{2} (q, K, q)) \mathcal{D}q
\,
\end{align}
%
as a \emph{probability density functional} \(\dd{P}[q]\), since 
\begin{enumerate}
    \item it is positive definite;
    \item it is normalized, as long as we set its integral, \(Z[0]\), to 1;
    \item it goes to zero as \(q \to \pm \infty \).
\end{enumerate}

If this is the case, then we ought to be able to compute the average value of a functional \(F[q]\) as 
%
\begin{align} \label{eq:average-of-a-functional}
\expval{F[q]} = \int F[q] \dd{P}[q] = \int \mathcal{D}q \exp(- \frac{1}{2} (q, K, q)) F[q]  
\,,
\end{align}
%
which we can generalize to any non-gaussian probability density functional by replacing the exponential \(\exp(- \frac{1}{2} (q, K, q))\) with a generic \(\mathcal{P}[q]\).

A useful kind of average we can compute is given by the \(N\)-point correlation function, 
%
\begin{align}
C^{(N)}(x_1, \dots, x_n) = \expval{q(x_1) \dots q(x_n)}
\,.
\end{align}

With the formula we gave earlier, this can be computed as 
%
\begin{align}
C^{(N)}(x_1, \dots , x_n)
= \int \mathcal{D}q \mathcal{P}[q] \prod_i q(x_i)
\,.
\end{align}

Here we can make use of a trick: going back to the Gaussian probability case, consider the functional derivative
%
\begin{align}
\eval{\frac{1}{i}\fdv{Z[J]}{J(x_1 )}}_{J = 0} &= \frac{1}{i} \eval{\fdv{}{J(x)}}_{J=0} \int \mathcal{D}q
\exp(- \frac{1}{2} (q, K, q) + i (J,q))  \\
&= \int \mathcal{D}q \exp(- \frac{1}{2} (q, K, q)) q(x_1 )
= \expval{q(x_1 )} = C^{(1)}(x_1 )
\,,
\end{align}
%
which actually holds for any probability density functional, we did not make use of the gaussianity.
So, in general we will be able to write 
%
\begin{align}
C^{(N)} (x_1 \dots x_n) = \frac{1}{i^{N}} \eval{\frac{ \delta^{n} Z[J]}{ \delta J(x_1 )\dots \delta J(x_n)}}_{J =0 }
\,.
\end{align}

The correlation functions, which as we discussed in an earlier section are crucial when discussing structure formation, can be ``simply'' calculated by functional differentiation as long as we have the generating functional \(Z[J]\).
This generating functional is very similar mathematically to a partition function in statistical mechanics, and it serves an analogous role: its derivatives allow us to characterize the dynamics of the system. 

Now, any functional \(\mathscr{F} [q]\) can be expressed through a functional Taylor series:
%
\begin{align}
\mathscr{F}[q] = \sum _{n=0}^{\infty } \frac{1}{n!}
\int \dd{x_1} \dots \dd{x_n} \eval{\frac{ \delta^{n} \mathscr{F}[q]}{ \delta q(x_1) \dots \delta q(x_n)}}_{q = 0} q(x_1) \dots q(x_n)
\,,
\end{align}
%
so if we compute the average value \(\expval{\mathscr{F}[q]}\) we find 
%
\begin{align}
\expval{\mathscr{F}[q]} &= 
\sum _{n=0}^{\infty } \frac{1}{n!}
\int \dd{x_1} \dots \dd{x_n} \eval{\frac{ \delta^{n} \mathscr{F}[q]}{ \delta q(x_1) \dots \delta q(x_n)}}_{q = 0} \underbrace{\expval{q(x_1) \dots q(x_n)}}_{= C^{(N)} (x_1 \dots x_n)}  \\
&=\sum _{n=0}^{\infty } \frac{(-i)^{n}}{n!}
\int \dd{x_1} \dots \dd{x_n} \eval{\frac{ \delta^{n} \mathscr{F}[q]}{ \delta q(x_1) \dots \delta q(x_n)}}_{q = 0} 
\eval{\frac{ \delta^{n} Z[J]}{ \delta J(x_1 )\dots \delta J(x_n)}}_{J =0 } \\
&= \mathscr{F}\qty[ -i \fdv{}{J}]\eval{ Z[J]}_{J=0}
\,.
\end{align}

\todo[inline]{Formally this makes sense, but what does it mean to calculate the field at a derivation operator? Should this just be interpreted as a shorthand for the Taylor expansion or is there more to it?}

An example: consider a Gaussian field whose partition function \(Z[J]\) is given by 
%
\begin{align}
Z[J] = \exp( - \frac{1}{2} (J, K^{-1}, J))
\,.
\end{align}

Then, as before we can calculate the correlation functions through functional derivatives: the first ones are 
%
\begin{align}
\expval{q(x)} &= \frac{1}{i} \eval{\fdv{Z[J]}{J(x)}}_{J=0}  \\
&= \eval{-i \int \dd{y} K^{-1}(x, y) J(y) \exp(- \frac{1}{2} (J, K^{-1}, J))}_{J =0} =0\\
\expval{q(y)q(x)} &= -\eval{\frac{ \delta^2 Z[J]}{ \delta  J(y) \delta  J (x)}}_{J=0} \\
&= -\eval{\qty(-K^{-1}(x, y) + \int \dd{z} \dd{w} K^{-1} (x, z)K^{-1} (y, w) J(w))\exp(- \frac{1}{2} (J, K^{-1}, J))}_{J =0}  \\
&= K^{-1} (x, y)
\,.
\end{align}

So, we have our result: \emph{for a Gaussian variable, the two-point correlation function is the inverse of the kernel}. 
A similar, albeit quite long, calculation allows us to compute the \(N\)-point correlation function for the same Gaussian variable: we expand the exponential in \(Z[J]\) in a power series, and when we differentiate it an even number of times we find  
%
\begin{align}
C^{2N}(x_1 \dots x_{2N}) &= \qty[K^{-1}(x_1, x_2 ) K^{-1} (x_3, x_4) \dots K^{-1} (x_{2N-1}, x_{2N})]_{\text{symmetrized}}
\,,
\end{align}
%
where ``symmetrized'' means that we must sum over all the permutations of the variables \(x_i\) in the argument of the inverse kernels; on the other hand, the odd correlation functions \(C^{2N+1}\) all vanish since they correspond to the integrals of odd functions over all space.

We can also apply this process in reverse: starting from the two-point correlation function we can reconstruct the kernel, and with it the probability density functional \(\dd{P}[q]\). 

In the Gaussian case, as long as we know the two-point function, which corresponds to the inverse kernel, we can reconstruct the \(N\)-point function. 
This can also be stated by saying that the ``irreducible'' \(N\)-point functions are all zero except for \(N=2\), since all the higher ones can be reduced to that one. 

We shall see that all the reduced \(N\)-point functions can be recovered starting from the \emph{generating functional} of connected correlation functions:
%
\begin{align}
\mathscr{W}[J] = \log Z[J]
\,,
\end{align}
%
through the expansion 
%
\begin{align}
\mathscr{W}[J] = \sum _{n=1}^{\infty } \frac{i^{n}}{n!} \int \dd{x_1} \dots \dd{x_n} C^{N}_{C} (x_1 \dots x_N) J(x_1) \dots J(x_n)
\,.
\end{align}

These connected correlation functions \(C^{N}_{C}\) are not (in principle) related to the \(C^{N}\) from before. 
\todo[inline]{What is the relation between them, though? What is the physical interpretation of these connected correlation functions?}
We could also have defined \(\mathscr{W}[J] = i \log Z[J]\), this is a matter of convention. 

Now, we define the \emph{classical field}
%
\begin{align}
q _{\text{cl}}(x) = \fdv{\mathscr{W}[J]}{J(x)}
\,,
\end{align}
%
and the effective action \(\Gamma [q _{\text{cl}}]\) as the Legendre transform of \(\mathscr{W}[J]\): 
%
\begin{align}
\Gamma [q _{\text{cl}}] = \mathscr{W}[J] - \int \dd{x} q _{\text{cl}} (x) J(x) 
\,,
\end{align}
%
from which we can then recover \(J(x)\) as 
%
\begin{align}
J(x) = - \fdv{\Gamma [q _{\text{cl}}]}{q _{\text{cl}}(x)}
\,.
\end{align}

In the Gaussian case we have 
%
\begin{align}
\mathscr{W}[J] = \log Z[J] = - \frac{1}{2} (J, K^{-1}, J)  
\,,
\end{align}
%
which, by direct comparison with the Taylor expansion, means that 
%
\begin{align}
C^{2}_{C} (x_1 , x_2 ) = K^{-1} (x_1 , x_2 )
\,,
\end{align}
%
while \(C^{N} \equiv 0\) for any \(N \neq 2\). 
Also, our expression for \(q _{\text{cl}}\) yields 
%
\begin{align}
q _{\text{cl}}(x) = -\int \dd{y} K^{-1}(x, y) J(y) 
\,,
\end{align}
%
from which we can express \(J(x)\) by using the direct kernel \(K(x, y)\): 
%
\begin{align}
\int \dd{x} q _{\text{cl}} (x) K (x, w) = - \int \dd{y} \dd{w} K^{-1}(x, y) K(x, w) J(y) = - \int \dd{w} \delta (y- w) J(y) = - J(w)
\,.
\end{align}

With an analogous procedure we can show that
%
\begin{align}
(J, K^{-1}, J) = (q _{\text{cl}}, K, q _{\text{cl}})
\,.
\end{align}

\todo[inline]{So in some sense \(J\) is a covariant vector while \(q\) is a contravariant one, right? can this be said in a better way?}

The effective action then reads
%
\begin{align}
\Gamma [q _{\text{cl}}] &= \eval{- \frac{1}{2} (J, K^{-1}, J) + (J, K^{-1}, J)}_{J = J(q _{\text{cl}})} \\
&= \frac{1}{2} (q _{\text{cl}}, K, q _{\text{cl}})
\,.
\end{align}

Now we have the tools to consider actual probabilities: starting from our classical field \(q\), we want to compute the probability that it takes on a certain value \(q \in (\alpha , \alpha + \dd{\alpha })\) at a point \(\overline{x}\): this is expressed with a probability density function in the form 
%
\begin{align}
\dv{P_q}{\alpha } =P_{q(x)} (\alpha; \overline{x})
\,.
\end{align}

We want to write this ``\(P(\alpha ) \dd{\alpha }\)'' in terms of the functional integral; in order to do so, we start from the Fourier transform 
%
\begin{align}
\int \dd{\beta } \exp(i \beta \varphi ) P_q (\beta ; \overline{x}) = \expval{e^{i \beta \varphi }}_{\beta }
\,.
\end{align}

The integral is a definite one, with bounds corresponding to the region in which \(P_q\) is nonzero, typically \(\mathbb{R}\).
The right-hand side is an average over the possible values taken on by the field \(q\) at the point \(\overline{x}\), but it can also be computed by averaging over the possible \emph{overall} field configurations, computed at a point \(\overline{x}\):
% This also holds if we average over the position \(x\): 
%
\begin{align}
\int \dd{\beta } \exp(i \beta \varphi ) P_q (\beta ; \overline{x}) = \expval{e^{i q(\overline{x}) \varphi }}_{q }
\,.
\end{align}

This will be much harder to compute: it is a proper functional integral, to be computed according to \eqref{eq:average-of-a-functional}; however it must give the same result.

% \todo[inline]{In the notes this is denoted as ``averaging over \(q\)'', but I do not see how that makes sense. Is \(\overline{x}\) a typical, generic position?}

If we take the Fourier antitrasform, we find 
%
\begin{align}
P_q (\alpha , \overline{x}) &= \frac{1}{2 \pi } \int \dd{\varphi } e^{-i \varphi \alpha } \expval{e^{i \varphi q (\overline{x})}}_q  \\
&= \frac{1}{2 \pi } \int \dd{\varphi } \expval{e^{i \varphi (q(\overline{x}) - \alpha )}}_q  \\
&= \expval{ \delta (q(\overline{x}) - \alpha ) }_q
\,.
\end{align}

This equation can be interpreted to mean the following: ``the probability that the field \(q\) is equal to \(\alpha \) at \(\overline{x}\) is given by the integral of the probabilities of all the field configurations which satisfy \(q(\overline{x}) = \alpha \)''.

This formalism can be generalized to \(N\)-point functions, the notation for the probability that for \(j = 1 \dots N\) the field \(q\) takes on the value \(\alpha _j\) at position \(x_j\) is as follows: 
%
\begin{align}
\dd{P}_q^{N} &= P_q \qty(\alpha_1 \dots \alpha _N; x_1 \dots x_N) \dd{\alpha_1 } \dots \dd{\alpha _N}  \\
&= P_q \qty([\alpha _N]; [x_N]) \dd{\alpha_1 } \dots \dd{\alpha _N}  \\
P_q \qty([\alpha _N]; [x_N]) 
&= \expval{\prod_{j=1}^{N} \delta (q(x_j) - \alpha _j)}_q
\,.
\end{align}

Statistical independence corresponds to the statement that the product can be brought out of the average, and we are interested in the general case, in which this does not happen.

The question we generally ask is: what is this probability? We can try to find an expression for it in terms of the partition function \(Z[J]\): we use once again the fact that 
%
\begin{align}
\delta (x) = \frac{1}{2 \pi } \int \dd{\varphi } e^{i \varphi x}
\,
\end{align}
%
to see that 
%
\begin{align}
P_q ([\alpha _N]; [X_N]) = \frac{1}{(2 \pi )^{N}} \int \dd{\varphi_1 } \dots \dd{\varphi _N} \exp(- i \sum _{j=1}^{N} \varphi _j \alpha _j)
\expval{\exp(i \sum _{j=1}^{N}\varphi _j q(x_j))}_q
\,.
\end{align}

This can be brought back to the partition function by making use of the fact that 
%
\begin{align}
Z[J] &= \expval{\exp(i (J, q))}_q  \\
&= \int \mathcal{D}q P[q] \exp( i \int \dd{x} J(x) q(x))
\,,
\end{align}
%
therefore 
%
\begin{align}
\expval{\exp(i \sum _{j=1}^{N}\varphi _j q(x_j))}_q
= Z \qty[ \sum _{j=1}^{N} \varphi _j \delta (x - x_j)] 
= Z[ \widetilde{J}_\varphi ]
\,,
\end{align}
%
where we have used the fact that 
%
\begin{align}
i \int \dd{x} q(x) \qty(\sum _{j=1}^{N} \varphi _j \delta (x-x_j))
= i \sum _{j=1}^{N} \varphi_j q(x_j) 
\,.
\end{align}

So, we can compute the probability that the field reaches the values \(\alpha _j\) at the points \(x_j\) as long as we can compute the partition function at \(Z[\widetilde{J}_\varphi ]\), and then integrate \(N\) times: 
%
\begin{align}
P_q ([\alpha _N], [x_N]) = \frac{1}{(2 \pi)^{N}} 
\int \dd{\varphi_1 } \dots \dd{\varphi _N }
\exp(-i \sum _{j=1}^{N} \varphi _j \alpha_j )
Z[\widetilde{J}_\varphi ]
\,.
\end{align}

Let us compute this for the case of a Gaussian random field \(q(x)\). The partition function evaluated at \(\widetilde{J}_\varphi = \sum _{j} \varphi _j \delta (x- x_j)\) is equal to 
%
\begin{align}
Z[\widetilde{J}_\varphi ] &= \exp( - \frac{1}{2} (\widetilde{J}_\varphi , K^{-1}, \widetilde{J}_\varphi )) \\
&= \exp( - \frac{1}{2} \sum _{i, j =1}^{N} \varphi _i \varphi _j K^{-1} (x_i, x_j))
\,.
\end{align}

Since it appears in the expression for the partition function, let us define the \emph{covariance matrix}
%
\begin{align}
M_{ij} = K^{-1}(x_i, x_j) = C^{(2)} (x_i, x_j)
\,,
\end{align}
%
so that the probability reads, using the usual result about Gaussian integrals,
%
\begin{align}
P_q ([\alpha _N], [x_N]) &= \frac{1}{(2 \pi)^{N}} 
\int \dd{\varphi_1 } \dots \dd{\varphi _N }
\exp(-i \sum _{j=1}^{N} \varphi _j \alpha_j )
\exp(- \frac{1}{2} \varphi_i M_{ij } \varphi _j)  \\
&= \frac{1}{\sqrt{(2 \pi)^{N} \det M}}
\exp(- \frac{1}{2} \alpha _i (M^{-1})_{ij} \alpha _j)
\,.
\end{align}

This is the standard expression for an \(N\)-variate Gaussian distribution whose covariance matrix is \(M_{ij}\). 

\textbf{Application to Brownian motion}. 

\textbf{Avoiding divergences: high- and low-pass filters}. 
If \(W_R (x)\) is our filter function, then the general correlation function reads 
%
\begin{align}
C^{(N)}_R (x_1 \dots x_N) &= \int \mathcal{D}q P[q] \prod_{r = 1}^{N}
\int \dd{y_r} q(y_r) W_R (\abs{y_r - x_r})  \\
&= \int \prod_{r=1}^{N} \dd{y_r} W_R(\abs{y_r - x_r}) C^{(N)} (y_1 \dots y_N)
\,.
\end{align}

The same holds for the \emph{connected} correlation functions, so we can calculate the \(C^{(N)}_{R, C}\): \emph{smooth, connected} \(N\)-point correlation functions. 

We can calculate these ``smoothed'' correlation functions through \(Z[J]\) by choosing \(J(x)\) in the form 
%
\begin{align}
J(x) =\int \dd{y} \varphi (x+y) W_R (y)
\,,
\end{align}
%
where \(\varphi \) is a generic function.

Since these regularized correlation functions do not explode at vanishing distances, we can define the \(N\)-th order \textbf{moment}:
%
\begin{align}
\expval{q_R^{N}} = C^{(N)}_R (x \dots x) 
\,
\end{align}
%
and the \(N\)-th order \textbf{cumulant}: 
%
\begin{align}
\expval{q_R^{(N)}}_C = C^{(N)}_{R, C} (x \dots x)
\,.
\end{align}

Due to homogeneity and isotropy, neither of these depends on \(x\). 
We expect these to diverge if we perform no filtering, which is equivalent to taking the filtering scale \(R \to 0\).
On the opposite limit, taking \(R \to \infty \) amounts to averaging over all space, so we expect \(\expval{q_R^{N}} \sim \expval{q}^{N}\), while \(\expval{q_R^{N}}_C \sim 0\). 

The moments of \(q_R\) can be obtained through the moment generating function 
%
\begin{align}
Z(\varphi ) = Z[J_\varphi (x) ] = Z[\varphi W_R (\abs{x})]
\,,
\end{align}
%
and we can define the \emph{cumulant} generating function \(W(\varphi )\) analogously. 
We can then define an effective action through a Legendre transform:
%
\begin{align}
\Gamma (q_{R, \text{cl}}) = W(\varphi ) - q_{R, \text{cl}} \varphi 
\,,
\end{align}
%
where the classical field \(q_{R, \text{cl}}\) is given by 
%
\begin{align}
q_{R, \text{cl}} = \dv{W(\varphi )}{\varphi }
\,.
\end{align}

The function \(Z(\varphi )\) is just the Fourier transform of \(P_R(\alpha , \overline{x})\): 
%
\begin{align}
Z(\varphi ) = \expval{e^{i \varphi q_R}}_q 
= \int  \dd{\alpha } e^{i \varphi \alpha } P_{q_R} (\alpha ; \overline{x}) 
\,,
\end{align}
%
so we can insert the explicit expression for the probability inside the inverse of this transform: 
%
\begin{align}
P_R(\alpha ; \overline{x}) &= \frac{1}{2 \pi } \int \dd{\varphi } e^{-i \alpha \varphi } Z(\varphi ) \\
&= \frac{1}{2 \pi } \int \dd{\varphi } e^{-i \alpha \varphi }
\int \mathcal{D}q P[q] \exp(i \varphi \int \dd{y} W_R (\abs{y - \overline{x}}) q(y) )
\,.
\end{align}

We can expand \(Z(\varphi )\) and \(W(\varphi )\) as 
%
\begin{align}
Z(\varphi ) &= 1 + \sum _{N=1}^{\infty } \frac{i^{N}}{N!} \varphi^{N} \expval{q_R^{N}} \\ 
W(\varphi ) &=  \sum _{N=1}^{\infty } \frac{i^{N}}{N!} \varphi^{N} \expval{q_R^{N}}_C  
\,,
\end{align}
%
\todo[inline]{why?}
therefore 
%
\begin{align}
\expval{q_R^{N}} = \int \dd{\alpha } \alpha^{N} P_{q_R} (\alpha ; \overline{x})
\,,
\end{align}
%
which clarifies what was meant by saying that these are \emph{moments}. 
\todo[inline]{Do we have a similar relation for the connected moments?}

\(P_{q_R}\) can be reconstructed starting from these moments. 

We can take a different path, by transforming the probability measure: we start from the probability density functional evaluated at \(q\), and calculate the expectation value of a generic functional \(F[q]\) as 
%
\begin{align}
\int \mathcal{D}q P[q] F[q]
\propto \int \mathcal{D}q_R P[(W^{-1}, q_R)] F[(W^{-1}, q_R)]
\,.
\end{align}

The proportionality is because we need to include a Jacobian determinant, however probability densities must always be normalized to 1 so any constant is inessential. 

Applying this to a Gaussian field amounts to mapping the kernel \(K\) to a ``smoothed'' kernel \(K_R\), which corresponds to the inverse of the smoothed two-point correlation function: \(\expval{q_R(x) q_R(y)}\). 

Now, we move to a more physical example. We consider a Gaussian field \(q\) in three dimensions, whose mean is zero and whose two-point function in Fourier space is 
%
\begin{align}
\expval{q(k) q(k')} = (2 \pi )^3 P(\abs{k}) \delta^{(3)} (k - k')
\,,
\end{align}
%
where \(P(\abs{k})\) is called the \textbf{power spectral density} of the field \(q\): it quantifies how much of the power of the field is transmitted at each frequency. 

The \textbf{Wiener–Khinchin theorem} tells us that the smoothed two-point function is given by 
%
\begin{align}
\expval{q_R (x) q_R(y)} &= G_R ( \abs{x-y}) = \frac{1}{(2 \pi )^3}
\int \dd[3]{k} P(k) \widetilde{W}^2_R(k) e^{i k \cdot (x-y)}  \\
&= \frac{1}{2 \pi^2} \int_0^{\infty } \dd{k } k^2 P(k) \widetilde{W}^2_R (k) j_0 (k \abs{x-y})
\,,
\end{align}
%
where \(j_0 \) is a Bessel function of the first kind. 
From the first expression we can read off the Fourier transform of \(K^{-1}_R\), whose inverse will then be 
%
\begin{align}
\widetilde{K}_R (k) = \frac{1}{P(k) \widetilde{W}_R^2(k)} 
\,.
\end{align}

Now, let us consider a typical power-spectrum: \(P(k) \propto k^{n}\). 
The smoothed two-point function will be asymptotically (in the \(r \ll R\) or \(r \gg R\) limits) be proportional to
%
\begin{align}
G_R (r) \propto \qty[\max(R, r)]^{- (n+3)}
\,.
\end{align}

The \(n = 0\) case is \textbf{white noise}, for which \(G_R(0) \sim R^{-3}\). 
\todo[inline]{Connection with a Poisson process\dots not clear}

The \(n = -3\) case is \textbf{flicker noise} corresponds to \(1/f\) noise in 1D.

In the \(n > 0\) case the smoothed two-point function \(G_R(r)\) must be equal to zero somewhere, since we have the constraint\footnote{This comes from the following line of reasoning: since the field has zero mean, 
%
\begin{align}
\int \dd[3]{y} \expval{q_R (x) q_R (y)} = \expval{q_R (x) \int \dd[3]{y} q_R(y)} = 0
\,.
\end{align}}
%
\begin{align}
\int_0^{\infty } \dd{r} r^2 G_R(r) = 0
\,.
\end{align}

\todo[inline]{What does this have to do with \(n>0\)? Isn't this constraint always present?}

The first zero-crossing of the correlation function, \(r_0 \) such that \(G_R(r_0 ) = 0\), is called the \textbf{coherence length}. 

If \(n < 0\), we have fractal behaviour: as long as \(G_R(r) \gg 1\), the fractal dimension is \(D_F = -n\). 
\todo[inline]{Clarify this\dots how would we show it?}

A common technique in QFT is the \textbf{Wick rotation} \(t \to -i t_E\), needed in order to move from a generally ill-defined oscillatory path integral of an exponential \(\exp(i S[q])\) in 4D space with an indefinite signature, to a Euclidean path integral of an exponential \(\exp(- S_E[q])\) in 4D space with a Cartesian signature, so that the D'Alambert operator is mapped to the Laplace-Beltrami operator.

In general, our probability measure can be written as 
%
\begin{align}
P[q] = \exp(- \frac{1}{2} (q, K, q) - V[q])
\,,
\end{align}
%
where the potential term encompasses all the non-quadratic parts of the probability. Linear terms can be removed with a change of variable, so we can expand it starting from third order: 
%
\begin{align}
V[q] = \sum _{n=3}^{\infty } \dd{x_1 } \dots \dd{x_n} K^{(n)} (x_1 \dots x_n) q(x_1 )\dots q(x_n)
\,.
\end{align}

Now, if we define 
%
\begin{align}
Z[J] = \int \mathcal{D}q \exp(- \frac{1}{2} (q, K, q) - V[q] + i (q, J) )
\,,
\end{align}
%
then we can expand the potential term perturbatively: 
%
\begin{align}
Z[J] &= \exp(- V \qty(-i \fdv{}{J})) 
\int \mathcal{D}q \exp(- \frac{1}{2} (q, K, q) + i (q, J) )  \\
&= \exp(- V \qty(-i \fdv{}{J}))
\underbrace{\exp(- \frac{1}{2} (J, K^{-1}, J))}_{Z_0 [J]}  \\
&= \sum _{n=0}^{\infty } \frac{(-)^{n}}{n!} \qty[V \qty(-i \fdv{}{J})]^{n} Z_0 [J]
\,.
\end{align}

With this approach, we can \textbf{recover the probability} of the field attaining a certain value as a function of the cumulants.

The \textbf{saddle point expansion} is the following: we want to compute an integral in the form 
%
\begin{align}
J = \int \dd{\tau } \exp(
    - f(\tau )
)
\,,
\end{align}
%
so we choose a \(\tau_0 \) such that \(f' (\tau_0 ) = 0\), and then approximate \(f\) up to second order: 
%
\begin{align}
J &\approx \int \dd{\tau } \exp(
    - f(\tau_0 ) - \frac{1}{2} f''(\tau_0 ) (\tau - \tau_0  )^2
)  \\
&\approx \sqrt{\frac{2 \pi }{f''(\tau_0 )}} \exp(- f(\tau_0 ))
\,.
\end{align}

We are effectively approximating the integrand as a Gaussian. 

We can apply this to the calculation of path integrals: if the probability \(P(q)\) can be written in terms of \(Z(\varphi ) = \exp(W(\varphi ))\), then we have 
%
\begin{align}
P(q) &= \frac{1}{2 \pi } \int \dd{\varphi } \exp(-i \varphi q + W(\varphi ))
\,.
\end{align}

The classical variable \(q _{R, \text{cl}} = \overline{q}\) is defined as \(\overline{q}= W' (\varphi ) \), and the effective action is written in terms of it: \(\Gamma (\overline{q}) = W(\varphi ) - \overline{q} \varphi \). Therefore, 
%
\begin{align}
\dv{\overline{q}}{\varphi } = W''(\varphi )
\,,
\end{align}
%
while the derivative of the classical action reads 
%
\begin{align}
\Gamma '' (\overline{q})
= \dv{}{\overline{q}}
\qty(- \varphi ) 
= - \qty[W''(\overline{q})]^{-1}
\,.
\end{align}

This means that \(\dd{\varphi } = - \Gamma'' (\overline{q}) \dd{\overline{q}}\), which allows us to change variable: 
%
\begin{align}
P(q) = -\frac{1}{2 \pi } \int \Gamma '' (\overline{q}) \dd{\overline{q}}
\exp( i q \Gamma '(\overline{q}) + \Gamma (\overline{q}) - \overline{q} \Gamma ' (\overline{q}))
\,.
\end{align}



\end{document}
