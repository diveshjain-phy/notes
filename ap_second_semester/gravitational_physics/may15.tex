\documentclass[main.tex]{subfiles}
\begin{document}

\section{Elements of data analysis}

\marginpar{Friday\\ 2020-5-15, \\ compiled \\ \today}

The issue is that our data are noise-dominated: extracting the signal is quite hard.
% About half of the people working on GW do this. 

The signal is something like 3 orders of magnitude below the noise.
Also, we want to say something about the source, extracting parameters like the masses of the BHs, the distance, the spins. 

We classify signals into: transient versus persistent (how long does the signal last?), and modeled versus unmodeled (do we know of a speficic waveform?).
 
\begin{enumerate}
    \item Transient modeled signals are usually coalescing binaries;
    \item persistent modeled signals can be binaries far from coalescence or rotating neutron stars;
    \item transient unmodeled signals can be supernovae or some other sources;
    \item persistent unmodeled signals are some form of stochastic background.
\end{enumerate}

\subsection{Matched filtering}

We will discuss matched filtering, which applies to \textbf{transient modeled} signals.
The assumption is that the signal is in the form 
%
\begin{align}
s(t) = \underbrace{h(t)}_{\text{known}} + \underbrace{n(t)}_{\text{noise}}
\,,
\end{align}
%
where by saying that \(h\) is known we mean that we have an idea of the waveform it should have, we do not know its precise parameters nor where it is in the signal. 

If we define the time averaging 
%
\begin{align}
\expval{x} = \frac{1}{T} \int_{0}^{T} x(t) \dd{t}
\,,
\end{align}
%
then we can  calculate \(\expval{s h}\), which is equal to \(\expval{h^2} + \expval{nh}\). 
We know that \(\expval{nh} \to 0\), since the noise is not correlated to the signal; specifically, \(\expval{nh} \sim T^{-1/2}\).

On the other hand, \(\expval{hh}\) will approach a constant nonzero value, so this integral will have a positive value if the signal is there.
% This grows much faster than the standard deviation of the uncorrelated signal. 
So, asymptotically we will expect \(\expval{sh} \to \expval{h^2}\).

Suppose we know what \(h(t)\) is, and we want to build a \textbf{linear filter} \(K(t)\) which returns a low value if the signal seems to not be in the data, and a high value if the signal seems not to be in the data.
The response of this filter will look like 
%
\begin{align}
s(t) \to \hat{s} = \int_{- \infty}^{\infty } \dd{t} s(t) K(t)
\,.
\end{align}

We assume that \(s(t) = \alpha h(t) + n(t)\), leaving the amplitude of the signal \(h(t)\) as a variable. 
We want to define the signal-to-noise ratio \(S / N\), where by \(S\) we mean the expectation value of \(\hat{s}\) when \(\alpha \neq 0\) and by \(N\) we mean the expectation value of \(\hat{s}\) when \(\alpha = 0\).

% \todo[inline]{What do we mean by ensemble average inside the time integral?}
The value of \(K(t)\) is fixed (we built the filter after all) so we can factor it out of ensemble averages. So, we find 
%
\begin{align}
S 
&= \expval{\hat{s}(t)}  = \int \dd{t} \expval{s(t)} K(t)  \\
&= \alpha \int \dd{t} \expval{h(t)} K(t) + \int \dd{t} \cancelto{}{\expval{n(t)}} K(t) \\
&= \alpha \int \dd{t} \expval{h(t)} \qty( \int \dd{f} e^{2 \pi i ft} K^{*}(f)) \\
&= \alpha \int \dd{f} K^{*} (f) \qty(\int \dd{t} e^{2 \pi i ft} \expval{h(t)}) \\
&= \alpha \int \dd{f} K^{*}(f) h(f)
\,,
\end{align}
%
where we have used the fact that, since \(K(t)\) is real (\(K(t) = K^{*}(t)\))
%
\begin{align}
K(t) 
= \int \dd{f} e^{-2 \pi i ft} K(f)
= \int \dd{f} e^{2 \pi i ft} K^{*}(f)
\,,
\end{align}
%
and we insert the latter expression so that we can recover the expression for the \emph{antitrasform} of \(h(f)\).
For the noise we need to compute the square modulus:
%
\begin{align}
N^2 &= \eval{\expval{\hat{s}^2(t)} - \cancelto{}{\expval{\hat{s}}^2}}_{\alpha = 0 } \\
&= \int \dd{t_1 } \dd{t_2 } K(t_1) K(t_2) \expval{n(t_1 ) n(t_2 )}  \\
&= \int \dd{t_1 } \dd{t_2 } K(t_1) K(t_2)
\int \dd{f_1 } \dd{f_2 } e^{2 \pi i t_1 f_1 } e^{2 \pi i t_2 f_2 } \expval{n(f_1 ) n(f_2 )}  \\
&= \int \dd{t_1 } \dd{t_2 } K(t_1) K(t_2)
\int \dd{f_1 } \dd{f_2 } e^{-2 \pi i t_1 f_1 } e^{2 \pi i t_2 f_2 } \underbrace{\expval{n^{*}(f_1 ) n(f_2 )}}_{= \frac{1}{2} \delta (f_1 - f_2) S_N(f_1 )} \\
&= \frac{1}{2} \int \dd{t_1 } \dd{t_2 } K(t_1) K(t_2) 
\int \dd{f_1 } S_n(f_1 ) \underbrace{e^{2 \pi i f_1 (t_2 - t_2 )}}_{= \delta (t_2 - t_1 )} \\
&= \frac{1}{2} \int \dd{t_1 } K(t_1 )K(t_1 ) \int \dd{f_1 } S_n(f_1 ) \\
&= \frac{1}{2} \int \dd{f_3 } \abs{K(f_3 )}^2 \int \dd{f_1 } S_n (f_1 ) \\
&= \frac{1}{2} \int \dd{f} \abs{K(f)}^2 S_n(f)
\,.
\end{align}

\todo[inline]{A step is not clear here either\dots}

Then, we can write 
%
\begin{align}
\frac{S}{N} = \alpha \frac{\int \dd{f} K^{*} (f) h(f)}{\sqrt{\int \dd{f} \abs{K(f)}^2 S_n(f) / 2}}
\,,
\end{align}
%
and we wish to optimize \(K\) in order to maximize this. 

We can express it in a clearer way if we define a scalar product for real functions of \(f\): 
%
\begin{align}
(A|B) = \Re \int \dd{f} \frac{A^{*}(f) B(f)}{S_n (f) / 2}
= 4 \Re \int_{0}^{\infty } \dd{f} \frac{A^{*}(f) B(f)}{S_n(f)}
\marginnote{The integrand is even since we are taking the real part and \(A(f) = A^{*}(-f)\).}
\,,
\end{align}
%
so we can write the SNR as  
%
\begin{align}
\frac{S}{N} = \alpha \frac{(u| h)}{(u | u)}
\,,
\end{align}
%
where we defined the vector \(u = S_n(f) K(f) / 2\).

So, we want \(u(f)\) to be parallel (i.\ e.\ proportional) to \(h\) in order to maximize the product: this means that \(S K/2 = \beta h\) for some \(\beta \), or  
%
\begin{align}
K(f) =  2 \beta \frac{h(f)}{S_n(f)}
\,.
\end{align}
%

This choice is called the \textbf{Wiener filter}, and with it we find the SNR 
%
\begin{align}
\frac{S}{N} = \alpha \frac{2 \beta \int \dd{f} h^{*}(f) h(f) / S_n(f) }{\sqrt{2 \beta^2 \int \dd{f} \abs{h(f)}^2 / S_n(f)}}  
= \sqrt{2} \alpha \sqrt{\int_{-\infty}^{\infty} \dd{f} \frac{\abs{h(f)}^2}{S_n(f)}}
\,.
\end{align}

This means that our best filter is \emph{not} simply \(h(f)\): we must weigh the signal by the detector noise is, the noisier regions are weighted less. 

We do not actually know \(h(t)\), since we do not know the time at which the signal will arriver: we can slide our filter over our signal, varying the time of arrival. 
We do this by defining 
%
\begin{align}
\hat{s} (\widetilde{t}) = \int \dd{t} s(t) K(t - \widetilde{t})
\,,
\end{align}
%
which will exhibit a peak for \(\widetilde{t}\) equal to the arrival time of the signal. 

In practice, we do not know the parameters of \(h(t)\) either. 
What we do is generate hundreds of thousands of filters spanning a reasonable parameter range and move them through the data.

We must decide on a coverage for our parameter space in order to run our filters, while still having a manageable number of iterations to do. 

Some problems with this: the detector noise is non-gaussian, its tails of extreme events are quite high. 
Also, the signal is often very weak compared to detector noise.

How to reject false alarms due to local noise? Coincidences between a detector network!
We allow time differences of \(L/c\) at most. 
% The amount of signal which is retained after these coincidences is very little. 

How to be very confident that two local sources of noise did not happen to coincide?
We delay the output intentionally by more than \(L / c\), in order to get an estimate of what we would see without GW signals (since we effectively eliminate any GW signals there could be in the data). 
This gives us the expected ``no-signal'' output.
This works as long as the events are rare. 

We can plot the probability density function we get for the detection statistic. 
If we leave the true signal in, this looks quite dirty since we have coincidences of noise with the GW; if we remove the GW, the PDF becomes a nice powerlaw.

The analysis must be done blind: we look at the delayed data stream first, and then we set the threshold. 

\todo[inline]{Do people use KDE in the estimation of PDFs?}

\subsection{Probability}

We use the Kolmogorov axioms: consider events belonging to the powerset of \(S\), then we say that 
\begin{enumerate}
    \item probabilities are positive;
    \item the probabilities of disjoint events are additive;
    \item the probability of \(S\) is 1.
\end{enumerate}

An implementation is the frequentist approach: we assign probabilities according to the frequency of occurrence of an event after many repetitions.

So, in this approach Bayes's theorem does not really make sense: what is the probability of a die being true, given that we have gotten 1006 times the number 1 over 6000 tries?
The die either is or isn't true. 

The alternate, Bayesian, approach is to define probabilities, instead, as subjective beliefs. 

Then, we can update our subjective belief about a hypothesis like 
%
\begin{align}
\mathbb{P} (\text{hyp} | \text{data}) = \frac{\mathbb{P}(\text{data} | \text{hyp}) \mathbb{P}(\text{hpy})}{\mathbb{P}(\text{data})}
\,.
\end{align}


\end{document}
