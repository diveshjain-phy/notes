\documentclass[main.tex]{subfiles}
\begin{document}

\chapter{Detectors}

\section{Noise theory}

\subsubsection{A simple experiment}

\marginpar{Friday\\ 2020-4-17, \\ compiled \\ \today}

In order to guide our description of noise theory, we start from a simple example: suppose we want to use a pendulum to measure the gravitational acceleration \(g\).
We want to use small oscillations, which should satisfy \(g \approx \omega^2L\), and we want to fit \(a(t)\).

Our signal can be very noisy compared to the theoretical waveform.
If we perform a \textbf{precision experiment} then the magnitude of the noise is comparable to (or smaller than) that of the signal: this is not a bad thing necessarily, it means we are operating at the very limit of our experimental capabilities. 

It \emph{is} a good idea to fit the whole timeseries instead of just counting oscillations, since we are using more data then. 

\subsection{Noise in experiments: a general formulation}

The physical system gives us a signal \(s(t)\), we have some measurement apparatus which gives us an output \(s'(t)\).

The output \(s'(t)\) can be also affected by noise: we can have physical noise \(n_1 (t)\) in the form of a \emph{random concurrent phenomenon}, noise \(n_2 (t)\) in the \emph{transducer}, and \(n_3 (t)\) in the \emph{readout}. The term \(n_1 \) comes from the phenomenon itself, the terms \(n_2\) and \(n_3 \) come from our measurement apparatus.  
This is a description of a very simple system, real-world experiments often are even more complicated than this! 

In our example: \(n_1 \) could be due to air currents or vibrations of the suspension points.
The transducer in our case is the accelerometer, so we can have input voltage instability, imperfect mechanical coupling, thermal vibrations contributing to \(n_2 \). 
In the readout noise \(n_3 \) we can have reference voltage instability, quantization if we digitize the signal, and electronic pick-up.

Also, in the signal \(s(t)\) there could be phenomena we do not want to measure and would regard as noise! 

% We want to distinguish the output signal \(s'\) from the output noise \(n'\).

% We define \textbf{precision experiments} as experiments where the signal amplitude is generally comparable or smaller than the noise amplitude.

We call \textbf{noise} any unwanted signal. 
In the classical realm, \(n(t)\) is the sum of deterministic processes, but in practice it is random since there are so many of them. We usually assume that they are zero-mean, while \(s(t)\) is macroscopically deterministic.

The question the experimenter must ask are: how do we \textbf{distinguish} signal from noise, and how can we \textbf{recover} the original signal? How do we \textbf{characterize} the noise in a given experiment? 

\subsection{Random processes}

A random variable is a number \(x\) associated to a possible experimental outcome. Any outcome has an associated probability.
In the continuous realm, this is described by a probability density function \(f(x)\), defined so that: 
%
\begin{align}
\mathbb{P} (x_0 \leq x \leq x_1 ) =  \int_{x_0 }^{x_1 } f(x) \dd{x}
\,,
\end{align}
%
which we can use to compute mean values:
%
\begin{align}
\int g(x) f(x) \dd{x} = \expval{g(x)}_{f}
\,,
\end{align}
%
and variances:
%
\begin{align}
\sigma^2  &= \int \dd{x} f(x) \qty(x - \expval{x})^2 = \expval{(x - \expval{x})^2} = \expval{x^2} - \expval{x}^2
\,.
\end{align}

Do note that at this point these are \emph{ensemble averages}: what we expect to be the average of a sample, \emph{not} time averages. 
The way to ideally compute the ensemble average of a signal would be to repeat the experiment an ``infinite number of times''. 

A random signal is a time series \(x(t)\); every time we repeat the experiment we get a new realization of \(x(t)\). 

Knowing \(x(\widetilde{t})\) at a specific time \(\widetilde{t}\) we only have \emph{partial} predictive power for \(x(\widetilde{t} + T)\). 

At a fixed time \(t\), the possible values of \(x(t)\) have a certain probability distribution \(f(x; t)\).
Then we can define the following functions of time: \(\mu (t) = \expval{x(t)}\) and \(\sigma^2(t)\). 

% Ideally these are averages over an infinite number of realizations.

Some properties our signal can have are: 
\begin{enumerate}
    \item \textbf{stochasticity}: the noise results from many uncorrelated processes, and correlation between the signal at a time \(t\) and at a time \(t + \Delta t\) decreases quickly with \(\Delta t\);
    \item \textbf{ergodicity}: this means that instead of an ensemble mean value we can compute a time average, and similarly for other statistical properties; 
    \item \textbf{stationarity}: statistical properties are time-independent;
    \item \textbf{gaussianity}: \(x(t)\) is normally distributed for a  fixed \(t\). 
\end{enumerate}

The assumption of gaussianity for a specific signal is qualitatively justified by the central limit theorem, as long as the noise signal is stochastic. 

The assumption of stationarity is not justified in general: there are several seasonal and daily phenomena which can alter the experimental apparatus; it is important to tread carefully here and only apply this assumption for small enough times.

Apart from this last consideration, these assumptions are almost always made in order to analyze the signal, but it is important to keep them in mind.

% We assume that the noise is \textbf{stochastic}, so it results from many different uncorrelated processes, and the correlation between the noise at a time \(t\) and \(t + \Delta t\) decreases quickly with \(\Delta t\). 

% \todo[inline]{So, we meand that the noise is high-frequency?}

% \textbf{Stationarity}: the apparatus is stable in time, and so are the processes that generate the noise.
% This is valid only for somewhat small periods. 

\subsection{Fourier transforms}

For square-integrable functions, 
%
\begin{align}
\int \abs{s(t)}^2 \dd{t} < \infty 
\,,
\end{align}
%
we define 
%
\begin{align}
s(\omega ) = \int s(t) e^{-i \omega t} \dd{t}
\qquad \text{and} \qquad
s(t) = \frac{1}{2 \pi } \int s(\omega ) e^{i \omega t} \dd{\omega } 
\,.
\end{align}

Important properties are linearity, the fact that in Fourier space derivatives become multiplication by \(i \omega \). 
Also, the convolution theorem: multiplication in time domain is the same as convolution in frequency domain: 
%
\begin{align}
\int s(t) q(t) e^{-i \omega t} \dd{t} = \frac{1}{2 \pi } \int s(\omega') q (\omega - \omega') \dd{\omega'}
\,.
\end{align}

The transform of the Dirac \(\delta(t) \) function is 1.
Almost any signal can be described as an infinite superposition of oscillatory terms. 

The transform is a complex-valued function, telling us the amplitude and phase of any component of the oscillation.

In practice, the frequency domain is very useful. Many interesting signals are well-defined in frequency.

Even multiple signals can be easily separated in frequency. 
Also, random noise is easier to characterize in frequency domain: the shape of the noise is interesting.

Some other properties: the Fourier transform preserves the energy (in the sense of the integral of the square modulus). 
It also preserves the information. It returns a Hermitian function. 

We have the uncertainty principle: 
%
\begin{align}
\Delta \omega \Delta t \geq \frac{1}{2} 
\,.
\end{align}

Depending on the physical characteristics of the signal, we should select a large observation time or a large frequency band.

\subsection{Power spectral density}

The phase of the noise will be completely different from one realization to the other.

We define the autocorrelation function: 
%
\begin{align}
R(t, t')  = \expval{x(t) x(t')} = R(\tau) 
\,,
\end{align}
%
where \(\tau = t - t'\). So, we define the power spectral density (PSD) as:
%
\begin{align}
S_x (\omega ) = \int  \dd{\tau } R(\tau ) e^{i \omega \tau } 
\,.
\end{align}

White noise has no correlation between a point and another: its auto-correlation function is a delta. It has all the frequency components. 
The autocorrelation function measures how fast the signal loses memory. 

A more loose and intuitive definition is the ensemble average: 
%
\begin{align}
S_x(\omega ) \delta (\omega - \omega') = \expval{x(\omega ) x^{*} (\omega')}
\,.
\end{align}

It is the average of the amplitude square, which is a power.

This is not the formal definition since \(x(\omega )\) may not exist.

If we build a window filter which only allows \([\omega_1 \leq \omega \leq \omega_2 ]\), then the residual power will be 
%
\begin{align}
P = \int_{\omega_1 }^{\omega_2 } S(\omega ) \dd{\omega }
\,.
\end{align}

For real signals, \(S\) is symmetric: \(S_x (-\omega ) = S_x (\omega )\). 
If we do not care about negative frequencies, we keep only the \([\omega \geq_0 ]\) region and multiply by 2.

The root mean square of the signal in time is the integral of the PSD: 
%
\begin{align}
\sigma^2_{x} = \int_{0}^{ \infty } S_x (\omega )
\,.
\end{align}

Here we are assuming that the signal has zero mean.
If two signals are uncorrelated, the power spectral density of their sum is the sum of their PSDs. 

The amplitude, or linear, spectral density, is \(\sqrt{S_x(\omega )}\). 

If our signal has units of \SI{}{m}, then \([S_x(\omega )] = \SI{}{m^2/Hz}\), so the linear PSD has units of \([\sqrt{S_x(\omega )}] \SI{}{m / \sqrt{Hz}}\).

In practice, we measure for a limited time \(T\). So, \(x\) will only have Fourier components at frequencies \(f_n = n / T\). The frequency resolution is \(\Delta f = 1 / T\). 

Having a measurement which is limited in time with a window \(w(t)\), we are actually measuring \(x(t) w(t)\): so, in frequency space we have 
%
\begin{align}
x _{\text{measured}} (\omega ) = \int \dd{\widetilde{\omega}} x(\widetilde{\omega}) w(\omega - \widetilde{\omega} ) 
\,,
\end{align}
%
so the Fourier transform of the window spreads out our signal.

We can deconvolve if we know what the window looks like. However, we do not usually do it. 
In fact, if we were to take out this windowing effect it would mean we are assuming that if we were to observe our signal for a longer time than we did we would see the same thing.
This might be justified sometimes, some other times it is not.

\end{document}
